{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONVOLUTIONAL NEURAL NETWORKS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.11.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP - 1: DATA PREPROCESSING"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROCESSING THE TRAINING SET"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is Image Augmentation, done to prevent overfitting.  \n",
    "Consists of transforming the images of the training set, so that the CNN model doesn't overlearn on the existing images.  \n",
    "By applying these transformations, we will get new images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255, shear_range = 0.2, zoom_range = 0.2,horizontal_flip=True)\n",
    "training_set = train_datagen.flow_from_directory('training_set',target_size=(64,64),batch_size=32,class_mode='binary')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target size is the dimensions of the images when they are fed into the cnn (length * breadth).  \n",
    "Batch size is the number of images you want to have in each batch.  \n",
    "Since we have binary outcome - cat or dog, class_mode is binary. The other form class_mode is categorical."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROCESSING THE TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_set = test_datagen.flow_from_directory('test_set',target_size=(64,64),batch_size=32,class_mode='binary')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 2: BUILDING THE CNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONVOLUTION"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are adding a convolution layer.    \n",
    "This convolution layer will be the object of a certain class - Conv2D class.  \n",
    "This class belongs to the layers module from keras library from tensorflow.  \n",
    "Filters/kernels are the number of feature detectors you want to apply to your images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = 3,activation = 'relu',input_shape = [64,64,3]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POOLING (MAX - POOLING)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pool size is the size of each pool.  \n",
    "Strides is the number of pixels by which you want to shift the frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADDING A SECOND CONVOLUTION LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = 3,activation = 'relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the second cnn layer is being added, the input_shape parameter is removed because it is entered only when you add the very first cnn layer. Here the second cnn layeris being added, so it is not required.  \n",
    "The input_shape parameter is used to connect the input layer to the very first layer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FLATTENING"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flattening the results of all these convolutions and poolings into a one-dimensional vector, which will become the input of a future fully connected neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FULL CONNECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units = 128, activation = 'relu'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OUTPUT LAYER"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are doing binary classification so we only need one neuron(unit).    \n",
    "We need only one neuron to encode the binary output (0 or 1)(cat ot dog)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 3: TRAINING THE CNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMPILING THE CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING THE CNN ON THE TRAINING SET AND EVALUATING IT ON THE TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 75s 282ms/step - loss: 0.6689 - accuracy: 0.5913 - val_loss: 0.6213 - val_accuracy: 0.6600\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 0.6173 - accuracy: 0.6607 - val_loss: 0.5924 - val_accuracy: 0.6925\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 60s 240ms/step - loss: 0.5672 - accuracy: 0.7074 - val_loss: 0.5811 - val_accuracy: 0.6955\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 60s 240ms/step - loss: 0.5408 - accuracy: 0.7211 - val_loss: 0.5085 - val_accuracy: 0.7525\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 45s 180ms/step - loss: 0.5137 - accuracy: 0.7498 - val_loss: 0.5320 - val_accuracy: 0.7350\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 0.4877 - accuracy: 0.7615 - val_loss: 0.5275 - val_accuracy: 0.7555\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 31s 124ms/step - loss: 0.4657 - accuracy: 0.7739 - val_loss: 0.4786 - val_accuracy: 0.7655\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 41s 163ms/step - loss: 0.4499 - accuracy: 0.7836 - val_loss: 0.4832 - val_accuracy: 0.7640\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 55s 220ms/step - loss: 0.4414 - accuracy: 0.7891 - val_loss: 0.4877 - val_accuracy: 0.7775\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 36s 146ms/step - loss: 0.4310 - accuracy: 0.8009 - val_loss: 0.4924 - val_accuracy: 0.7525\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 34s 137ms/step - loss: 0.4030 - accuracy: 0.8149 - val_loss: 0.4988 - val_accuracy: 0.7650\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 61s 244ms/step - loss: 0.3989 - accuracy: 0.8154 - val_loss: 0.4620 - val_accuracy: 0.7865\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 0.3832 - accuracy: 0.8282 - val_loss: 0.4794 - val_accuracy: 0.7835\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 36s 145ms/step - loss: 0.3640 - accuracy: 0.8340 - val_loss: 0.4650 - val_accuracy: 0.7920\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 0.3463 - accuracy: 0.8484 - val_loss: 0.4982 - val_accuracy: 0.7745\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 0.3314 - accuracy: 0.8555 - val_loss: 0.4751 - val_accuracy: 0.8025\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 44s 175ms/step - loss: 0.3213 - accuracy: 0.8629 - val_loss: 0.4884 - val_accuracy: 0.7965\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 0.3013 - accuracy: 0.8689 - val_loss: 0.4820 - val_accuracy: 0.8000\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 0.2875 - accuracy: 0.8761 - val_loss: 0.4872 - val_accuracy: 0.7995\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 60s 240ms/step - loss: 0.2751 - accuracy: 0.8841 - val_loss: 0.5671 - val_accuracy: 0.7820\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 43s 170ms/step - loss: 0.2649 - accuracy: 0.8911 - val_loss: 0.5407 - val_accuracy: 0.7885\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 43s 173ms/step - loss: 0.2462 - accuracy: 0.8906 - val_loss: 0.5515 - val_accuracy: 0.7890\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 40s 161ms/step - loss: 0.2340 - accuracy: 0.9040 - val_loss: 0.7701 - val_accuracy: 0.7385\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 41s 163ms/step - loss: 0.2189 - accuracy: 0.9091 - val_loss: 0.5858 - val_accuracy: 0.7855\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 35s 139ms/step - loss: 0.2002 - accuracy: 0.9169 - val_loss: 0.6152 - val_accuracy: 0.7765\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1894e4dbeb0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x = training_set,validation_data=test_set, epochs = 25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 48ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras.utils as image\n",
    "test_image = image.load_img('single_prediction/cat_or_dog_7.jpg',target_size = (64,64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "#The last expand_dims is used to add a dimension according to the batch. \n",
    "result = cnn.predict(test_image)\n",
    "training_set.class_indices\n",
    "if result[0][0] ==1:\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9404ea6465ccf5b5974759fd7d25b8ffc49d5b7ab840d7c5342355a540530145"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
